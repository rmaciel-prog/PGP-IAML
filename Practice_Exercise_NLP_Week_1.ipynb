{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice Exercise: NLP Week-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5skCvmbYujon",
        "colab_type": "text"
      },
      "source": [
        "Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPqxsUmuvJv",
        "colab_type": "text"
      },
      "source": [
        "- NLTK - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, and beyond\n",
        "\n",
        "- BeautifulSoup - BeautifulSoup is a useful library for extracting data from HTML documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qseduth-xXTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries.\n",
        "import re, string, unicodedata\n",
        "import nltk                                   # Natural language processing tool-kit\n",
        "\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup                 # Beautiful soup is a parsing library that can use different parsers.\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet    # Stopwords, and wordnet corpus\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z56oRBGgu2ZW",
        "colab_type": "text"
      },
      "source": [
        "We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBoMiEguukgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"<h1>This is the title</h1>\n",
        "            <b>This is bold text</b>\n",
        "            <i>This is italicized Text</i>\n",
        "            <img src=\"another html tag\"/>\n",
        "            <a href=\"Apart from the others\"> This is also here!</a>\n",
        "            “Love all, trust a few, do wrong to none.” \n",
        "            ― William Shakespeare, All's Well That Ends Well\n",
        "\n",
        "            “All the world's a stage,\n",
        "            And all the men and women merely players;\n",
        "            They have their exits and their entrances;\n",
        "            And one man in his time plays many parts,\n",
        "            His acts being seven ages.” \n",
        "            ― William Shakespeare, As You Like It\n",
        "\n",
        "            \"How old are you,\" asked Jem, \"four-and-a-half?\"\n",
        "\n",
        "            \"Goin' on seven.\"\n",
        "\n",
        "            \"Shoot no wonder, then,\" said Jem, jerking his thumb at me. \"Scout yonder's been readin' ever since she was born, \n",
        "            and she ain't even started to school yet. You look right puny for goin' on seven.\"\n",
        "\n",
        "            \"I'm little but I'm old,\" he said.\n",
        "            - To Kill a Mockingbird\n",
        "\n",
        "            Le dîner, Clémence, Anaïs, Raphaël, Voilà !\n",
        "\n",
        "            something... is! not right() with.,; this :: line.\n",
        "            \n",
        "            &nbsp;&nbsp;\n",
        "            \n",
        "            11    42   1024   2048\n",
        "            {{There are double curly braces.}}\n",
        "            {Here are single curly braces.}\n",
        "            </body>\n",
        "            </html>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj6-7g_WwbbI",
        "colab_type": "text"
      },
      "source": [
        "# Noise Removal\n",
        "\n",
        "Let's define noise removal as text-specific normalization tasks which often take place prior to tokenization. \n",
        "- While the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.\n",
        "\n",
        "Noise removal tasks could include:\n",
        "\n",
        "- Removing text file headers, footers\n",
        "- Removing HTML, XML, etc. markup and metadata\n",
        "- Extracting valuable data from other formats, such as csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OybtmpuJxAbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to remove all the html tags from the text string. And print the processed text.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE9JDVmMGR_t",
        "colab_type": "text"
      },
      "source": [
        "While not mandatory to do at this stage prior to tokenization but:\n",
        "- Replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\"\n",
        "- It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIZoPnS-GmGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the code to replace all the contractions. (I'm  ==>>  I am and so on.) [Hint: use contractions library.]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk7FdvKSHM-u",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        " \n",
        "- Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. \n",
        "- Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. \n",
        "- Further processing is generally performed after a piece of text has been appropriately tokenized. \n",
        "- Tokenization is also referred to as text segmentation or lexical analysis.\n",
        "- Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words.\n",
        "\n",
        "### For our task, we will tokenize our sample text into a list of words. This is done using NTLK's word_tokenize() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6isuJOhmxHmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the text and print.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6txvsPaw6FAT",
        "colab_type": "text"
      },
      "source": [
        "# Normalization\n",
        "\n",
        "- converting all text to the same case (upper or lower), removing punctuation,  and so on.\n",
        "\n",
        "- Steps:\n",
        "  - Removal of non-ASCII characters.\n",
        "  - Conversion of all characters to lowercase.\n",
        "  - Removal of Punctuation.\n",
        "  - Stop word removal.\n",
        "  - Stemming / Lemmatization\n",
        "\n",
        "- After tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgLuyWs2IchN",
        "colab_type": "text"
      },
      "source": [
        "Converting all words to lowercase and removing punctuations.\n",
        "\n",
        "**Stemming:** Converting the words into their base word or stem word ( Ex - tastefully, tasty, these words are converted to stem word called 'tasti'). This reduces the vector dimension because we dont consider all similar words\n",
        "\n",
        "**Stopwords:** Stopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change.\n",
        "\n",
        "Ex - **This pasta is so tasty** ==> **pasta tasty** ( This , is, so are stopwords so they are removed)\n",
        "\n",
        "Hint:\n",
        "\n",
        "- Use regular expressions to remove punctuations.\n",
        "\n",
        "To see all the steps, run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbp1Y-OVyAi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write function to remove non-ASCII characters from the list of tokenized words.\n",
        "\n",
        "\n",
        "# Write function to convert all the characters from the list of tokenized words.\n",
        "\n",
        "\n",
        "# Write function to remove punctuations from the list of tokenized words.\n",
        "\n",
        "\n",
        "# Write function to remove stopwords from the list of tokenized words.\n",
        "\n",
        "\n",
        "# Write function to convert to stem words from the list of tokenized words.\n",
        "\n",
        "\n",
        "# Write function to lemmatize the words from the list of tokenized words.\n",
        "\n",
        "\n",
        "# write a function to perform all the above steps.\n",
        "\n",
        "\n",
        "# write the code to execute the function which has all the above steps combined.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTaG2mA7FOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}